{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RNN によるテキスト生成  \\|  TensorFlow Core](https://www.tensorflow.org/tutorials/text/text_generation?hl=ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シェイクスピアデータセットのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# ファイル中のユニークな文字の数\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストのベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれの文字からインデックスへの対応表を作成\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測タスク\n",
    "\n",
    "ある文字、あるいは文字列が与えられたとき、もっともありそうな次の文字はなにか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練用サンプルとターゲットを作成\n",
    "\n",
    "`seq_length` 個の文字があるとき、その次に現れる文字は何か？\n",
    "\n",
    "例えば、テキストが「Hello」、seq_length が 4 だとすると、入力データが「Hell」で教師データが「ello」となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# ひとつの入力としたいシーケンスの文字数としての最大の長さ\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# 訓練用サンプルとターゲットを作る\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初のサンプルの入力とターゲット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練用バッチの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バッチサイズ\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# データセットをシャッフルするためのバッファサイズ\n",
    "# （TF data は可能性として無限長のシーケンスでも使えるように設計されています。\n",
    "# このため、シーケンス全体をメモリ内でシャッフルしようとはしません。\n",
    "# その代わりに、要素をシャッフルするためのバッファを保持しています）\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの構築\n",
    "\n",
    "- Embedding: 入力層。それぞれの文字を表す数を embedding_dim 次元のベクトルに変換する、訓練可能な参照テーブル\n",
    "- GRU : サイズが units=rnn_units のRNNの一種（ここに LSTM レイヤーを使うこともできる）（GRUはLSTMに比べて学習パラメータが少なく、計算時間が短い特徴がある）\n",
    "- Dense: vocab_size 個の出力を持つ、出力層\n",
    "\n",
    "出力の形は `(バッチサイズ, 文字列の長さ, 語彙数) = (64, 100, 65)` だが、モデルはどのような長さの入力もできるので、`(64, None, 65)` となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 文字数で表されるボキャブラリーの長さ\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 埋め込みベクトルの次元\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN ユニットの数\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=(batch_size, None)),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルから実際の予測を得るには出力の分布からサンプリングを行う必要がある。\n",
    "分布からサンプリングのは、分布の argmax をとっただけでは、モデルは簡単にループしてしまうから。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 34, 24,  3, 23, 57, 30, 55,  4, 45, 41, 38, 51,  1, 34, 44, 23,\n",
       "       35,  7, 40, 34, 61, 16, 26, 33, 53, 27,  0, 30, 57, 25, 48, 23, 49,\n",
       "       24, 11,  9, 27, 13, 59, 29, 21, 52, 11, 63,  1, 10, 57, 50,  8, 22,\n",
       "       64, 47, 29, 58, 22, 53, 34, 32, 23, 57, 61, 32, 52,  1, 18, 16, 18,\n",
       "       30, 29,  1, 12, 17,  3, 22, 64,  9, 58, 41, 24, 44, 21,  0, 47, 60,\n",
       "        5, 28, 49, 52, 36, 42, 60, 15, 48, 41, 22, 48, 48, 29, 63])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "# 次の文字をランダムに選ぶ (サンプリング)\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "# Flatten\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練前のモデルによる予測テキストをデコードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'icularly and to all the Volsces\\nGreat hurt and mischief; thereto witness may\\nMy surname, Coriolanus:'\n",
      "\n",
      "Next Char Predictions: \n",
      " \"gVL$KsRq&gcZm VfKW-bVwDNUoO\\nRsMjKkL;3OAuQIn;y :sl.JziQtJoVTKswTn FDFRQ ?E$Jz3tcLfI\\niv'PknXdvCjcJjjQy\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの訓練\n",
    "\n",
    "- 損失関数 : `from_logits=True` でロジスティック回帰 (0〜1にする?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1755595\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チェックポイントの構成\n",
    "\n",
    "訓練中にチェックポイントを保存するようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チェックポイントが保存されるディレクトリ\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# チェックポイントファイルの名称\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 172 steps\n",
      "Epoch 1/10\n",
      "172/172 [==============================] - 11s 62ms/step - loss: 2.6526\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.9388\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.6744\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.5302\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 9s 54ms/step - loss: 1.4447\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.3861\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.3412\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.3006\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 10s 59ms/step - loss: 1.2669\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 9s 55ms/step - loss: 1.2324\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, \n",
    "                    epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成\n",
    "\n",
    "### 最終チェックポイントの復元\n",
    "\n",
    "予測ステップを単純にするため、バッチサイズ 1 を使用する。\n",
    "\n",
    "RNN が状態をタイムステップからタイムステップへと渡す仕組みのため、モデルは一度構築されると固定されたバッチサイズしか受け付けないため、モデルを異なる batch_size で実行するためには、モデルを再構築し、チェックポイントから重みを復元する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None])) # 入力の形\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測ループ\n",
    "\n",
    "1. 開始文字列を選択し、RNN の状態を初期化して、生成する文字数を設定する\n",
    "2. 開始文字列と RNN の状態を使って、次の文字の予測分布を取得する\n",
    "3. 予測分布から次の文字を求め、この文字をモデルの次の入力にする\n",
    "4. モデルによって返された RNN の状態はモデルにフィードバックされるため、1つの文字だけでなく、より多くのコンテキストを持つことになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 評価ステップ（学習済みモデルを使ったテキスト生成）\n",
    "\n",
    "  # 生成する文字数\n",
    "  num_generate = 1000\n",
    "\n",
    "  # 開始文字列を数値に変換（ベクトル化）\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 結果を保存する空文字列\n",
    "  text_generated = []\n",
    "\n",
    "  # 低い temperature　は、より予測しやすいテキストをもたらし\n",
    "  # 高い temperature は、より意外なテキストをもたらす\n",
    "  # 実験により最適な設定を見つけること\n",
    "  temperature = 1.0\n",
    "\n",
    "  # ここではバッチサイズ　== 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # バッチの次元を削除\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # カテゴリー分布をつかってモデルから返された文字を予測 \n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # 過去の隠れ状態とともに予測された文字をモデルへのつぎの入力として渡す\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: no measure,\n",
      "But his noble cousin?\n",
      "\n",
      "Second Keeper:\n",
      "But where he knew not that knows not this to leath which I do not much\n",
      "doing in llin person, and her falsehear's makes this much:\n",
      "I'll kiss my speech ank worship to God Eew purlips;\n",
      "Without my uncles from his boundain than thousain?\n",
      "Here refaise the time amongst the Volsces and my shoulder,\n",
      "Or else a heavenly sprink in honset.\n",
      "Good one fearful,\n",
      "I'll bear your husband cries 'God save this? Was a much son,\n",
      "Not let him ne'er down this newly heard;\n",
      "Gromits, recapery are dreams;\n",
      "We met, as he did such answer from a sight.\n",
      "\n",
      "KATHARINA:\n",
      "Will't Warwick?\n",
      "\n",
      "ANGELO:\n",
      "Itwas not but within my grave:\n",
      "If I smoater, weapons of these namies,\n",
      "From traitors oath lies: therefore shall you this is traitor.\n",
      "\n",
      "VOLUMNIA:\n",
      "Here, make her sport, if he be march'd together?\n",
      "Alas! please you, sir: I sprinkshine, she shall be strange\n",
      "Myself concealing his grief and let your grave.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "It was his heart; I had as law makes the ooth of his,\n",
      "And dable tale as\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
